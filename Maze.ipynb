{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\VS-Code\\Reinforcement-Learning\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From f:\\VS-Code\\Reinforcement-Learning\\.venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From f:\\VS-Code\\Reinforcement-Learning\\.venv\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mazegen import MazeEnv\n",
    "import random\n",
    "from dqnAgent import DQN\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 3, activation='relu', input_shape=(128,128,3)),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(4, activation = 'linear')\n",
    "        ])\n",
    "\n",
    "agent=DQN(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=MazeEnv(size=4)\n",
    "test_env=MazeEnv(size=4, render_mode=\"human\")\n",
    "N_ACTION=env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration\n",
    "epsilon=1\n",
    "EPSILON_DECAY=0.995\n",
    "MIN_EPSILON=0.001\n",
    "\n",
    "# Visualization\n",
    "PREVIEW_TRAIN=False\n",
    "PREVIEW_EVAL=False\n",
    "\n",
    "# Episodes\n",
    "N_EPISODES=2000\n",
    "AGGREGATE_STATS_EVERY=250\n",
    "LOAD_PATH=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/data-science-in-your-pocket/advantage-actor-critic-a2c-algorithm-in-reinforcement-learning-with-codes-and-examples-using-e810273c0c9e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_PATH != \"\":\n",
    "    agent.load(LOAD_PATH)\n",
    "current_ep_rewards=[]\n",
    "ep_rewards=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\VS-Code\\Reinforcement-Learning\\.venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From f:\\VS-Code\\Reinforcement-Learning\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "AGGREGATING EPISODE:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2000 [00:08<4:51:56,  8.76s/episodes]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward:  -2.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 13/2000 [01:58<4:34:17,  8.28s/episodes]"
     ]
    }
   ],
   "source": [
    "agent.fill_min_memory(env)\n",
    "for episode in tqdm(range(N_EPISODES), ascii=True, unit='episodes'):\n",
    "\n",
    "    # Set reward to initial values\n",
    "    episode_reward=0\n",
    "\n",
    "    # Reset Environment\n",
    "    current_state, _ =env.reset()\n",
    "\n",
    "    # Iterate until episode end\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        # We use epsilon greedy policy, we take a random number and compare it to epsilon\n",
    "        if random.random() > epsilon:\n",
    "            # Choose action from q table\n",
    "            action = np.argmax(agent.get_qtable(np.array([current_state])))\n",
    "        else:\n",
    "            # Random action\n",
    "            action = random.choice([i for i in range(N_ACTION)])\n",
    "\n",
    "        # Perform the step and get data\n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # Sum the reward\n",
    "        episode_reward+=reward\n",
    "\n",
    "        # Show preview if conditions are met\n",
    "        if PREVIEW_TRAIN:\n",
    "            env.render()\n",
    "\n",
    "        # Every step we update replay memory\n",
    "        agent.append_replay_memory((current_state, action, new_state, reward, done))\n",
    "        agent.train(done)\n",
    "\n",
    "        current_state = new_state\n",
    "\n",
    "    current_ep_rewards.append(episode_reward)\n",
    "\n",
    "    if episode % AGGREGATE_STATS_EVERY == 0:\n",
    "        print(\"AGGREGATING EPISODE: \", episode) \n",
    "        # Get the average, minimum maximum reward across AGGREGATE_STATS_EVERY episodes\n",
    "        average_reward = sum(current_ep_rewards) / len(current_ep_rewards)\n",
    "        min_reward = min(current_ep_rewards)\n",
    "        max_reward = max(current_ep_rewards)\n",
    "\n",
    "        ep_rewards.append([average_reward, min_reward, max_reward])\n",
    "        current_ep_rewards=[]\n",
    "\n",
    "        # Evaluation\n",
    "        print(\"Reward: \", agent.evaluate_policy(test_env, render=PREVIEW_EVAL))\n",
    "        agent.save(\"best_model_maze.keras\")\n",
    "        \n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"best_model_maze.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.evaluate_policy(test_env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
